{
  
    
        "post0": {
            "title": "Applications of SVD",
            "content": "MOTIVATION . The singular value decomposition or SVD is a real or complex matrix factorization in linear algebra. SVD has been widely used in various areas, such as Moore-Penrose inverse, linear system of equations, PCA, and image compression. In this project, we are going to understand some intuitive interpretations of SVD and explore some useful applications of SVD in different fields. . 1. Diagonalization . Definition: Suppose $ mathbf{A}$ is a $n times n$ matrix, where A has $n$ linearly independent eigenvectors $x$, then $$A = XDX^{-1}$$ . where $$A = left[ begin{matrix} lambda_1 &amp; &amp; &amp; lambda_2 &amp; &amp; &amp; &amp; ddots &amp; &amp; &amp; &amp; lambda_n end{matrix} right]$$ and $$X = left[ begin{matrix} overrightarrow{x_1} &amp; overrightarrow{x_2} &amp; ldots &amp; overrightarrow{x_n} end{matrix} right]$$ . As shown above, the columns of $X$ are the linearly independent normalized eigenvectors $ overrightarrow{x_1}, overrightarrow{x_2}, ldots, overrightarrow{x_n}$ of the matrix A (which guarantees that $X^{-1}$ exists) and D is a diagonal matrix with the eigenvalues $ lambda_1, lambda_2, ldots, lambda_n$ of the matrix $A$. . If a $n times n$ symmetric matrix $ mathbf{A}$ has n distinct eigenvalues, then A is diagonalizable. | . Proof: Suppose $ lambda, u$ and $ mu, v$ are eigenpairs of the matrix: . $$ Au = lambda u$$ and $$Av = mu v$$ . Then since A is symmetric $ Rightarrow$ $A^{T} = A$ . $$Au = lambda u$$ $$v. Au = lambda v.u$$ $$A^{T} v. u = lambda v.u$$ $$A v. u = lambda v.u$$ $$ mu v u = lambda v.u$$ $$( mu - lambda) v.u = 0$$ . Considering $ mathbf{A}$ has n distinct eigenvalues $ Rightarrow$ $ mu - lambda neq 0 Rightarrow v.u = 0$ . Therefore, eignvectors are orthogonal $ rightarrow$ $ mathbf{A}$ is diagonalizable. . If a $n times n$ matrix $ mathbf{A}$ has less than n linearly independent eigenvectors, then A is not diagonalizable which is called defective. | . Considering the indenitty matrix: $$I = left[ begin{matrix} 1 &amp; 0 &amp; 0 0 &amp; ddots &amp; 0 0 &amp; 0 &amp; 1 end{matrix} right]$$ which has eigenvalues $ lambda_1 = lambda_2 = cdots = lambda_n = 1$, and n linearly independent eigenvectors: $U = left[ begin{matrix} 1 &amp; 0 &amp; 0 0 &amp; ddots &amp; 0 0 &amp; 0 &amp; 1 end{matrix} right]$. Even though the matrix I is diagonalizable but does not have distinct eigenvalues. As we can see, $ lambda = 1$ is an eigenvalue with algebraic multiplicity equal to n and $ lambda = 1$ is an eigenvalue with geometric multiplicity equal to n | . If geometirc multiplicity &lt; algebraic multiplicity, the eigenvalue is called defective. | . 2 Singular Value Decomposition . Singular Value decomposition generalies the diagonalization. The $ mathbf{singular}$ $ mathbf{value}$ $ mathbf{decomposition}$ (SVD) is a factorization of a $m times n$ matrix A into $$A = U Sigma V^{T}$$ . $$A = left[ begin{matrix} vdots &amp; ldots &amp; vdots u_1 &amp; ldots &amp; u_m vdots &amp; ldots &amp; vdots end{matrix} right] left[ begin{matrix} sigma_1 &amp; &amp; &amp; ddots &amp; &amp; &amp; sigma_n &amp;&amp;0 &amp;&amp; vdots &amp;&amp;0 end{matrix} right] left[ begin{matrix} ldots &amp; v_1^{T} &amp; ldots vdots &amp; vdots &amp; vdots ldots &amp; v_n^{T} &amp; ldots end{matrix} right]$$ where $U$ is a $m times m$ orthogonal matrix, $V^{T}$ is a $n times n$ orthogonal matrix ($ sigma_1 geq sigma_2 geq sigma_3 geq ldots$), and $ Sigma$ is a $m times n$ diagonal matrix. . 2.1 First, Assume that A is a Square Matrix (m = n) . The matrix $A$ has the singluar value decomposition $A = U Sigma V^{T} $ and $A^{T} = (U Sigma V^{T})^{T} = V Sigma^{T} U^{T}$, then we can have: . First Case $(A^{T}A)$: $$A^{T}A = (V Sigma^{T} U^{T})(U Sigma V^{T}) = V Sigma^{T} U^{T} U Sigma V^{T} = V Sigma^{T} I Sigma V^{T} = V Sigma^{T} Sigma V^{T} = V Sigma^{2} V^{T}$$ . $$A^{T}A = V Sigma^{2} V^{T}$$ . Considering the definition of diagonalization, we have $Z = XDX^{-1} = XDX^{T}$ . Let $C = A^{T}A$, we can get $C = V Sigma^{2} V^{T}$ $ Rightarrow$ $$V = left[ begin{matrix} vdots &amp; ldots &amp; vdots v_1 &amp; ldots &amp; v_n vdots &amp; ldots &amp; vdots end{matrix} right], Sigma^{2} = left[ begin{matrix} sigma_1^{2} &amp; &amp; &amp; ddots &amp; &amp; &amp; sigma_n^{2} end{matrix} right] = left[ begin{matrix} lambda_1 &amp; &amp; &amp; ddots &amp; &amp; &amp; lambda_n end{matrix} right], V^{T} = left[ begin{matrix} ldots &amp; v_1^{T} &amp; ldots vdots &amp; vdots &amp; vdots ldots &amp; v_n^{T} &amp; ldots end{matrix} right] $$ . where $ forall i$, we will have $ lambda_i = sigma_i^{2}$ and $ sigma_i = sqrt{ lambda_i}$ . Therefore, by looking at the eigenpairs with respect to $A^{T}A$, we can conclude that: . Columns of $V$ are the eigenvectors of $A^{T}A$. | $ Sigma^{2}$ are the eigenvalues of $A^{T}A$. | . Second Case $(AA^{T})$: $$A A^{T} = (U Sigma V^{T})(V Sigma^{T} U^{T}) = U Sigma^{T} V^{T} V Sigma U^{T} = U Sigma^{T} I Sigma U^{T} = U Sigma^{T} Sigma U^{T} = U Sigma^{2} U^{T}$$ . $$AA^{T} = U Sigma^{2} U^{T}$$ . Let $D = A^{T}A$, we can get $D = U Sigma^{2} U^{T}$ $ Rightarrow$ $$U = left[ begin{matrix} vdots &amp; ldots &amp; vdots u_1 &amp; ldots &amp; u_n vdots &amp; ldots &amp; vdots end{matrix} right], Sigma^{2} = left[ begin{matrix} sigma_1^{2} &amp; &amp; &amp; ddots &amp; &amp; &amp; sigma_n^{2} end{matrix} right] = left[ begin{matrix} lambda_1 &amp; &amp; &amp; ddots &amp; &amp; &amp; lambda_n end{matrix} right], U^{T} = left[ begin{matrix} ldots &amp; u_1^{T} &amp; ldots vdots &amp; vdots &amp; vdots ldots &amp; u_n^{T} &amp; ldots end{matrix} right] $$ . where $ forall i$, we will have $ lambda_i = sigma_i^{2}$ and $ sigma_i = sqrt{ lambda_i}$ . Similarly, at this time we can conclude that: . Columns of $U$ are the eigenvectors of $AA^{T}$. | $ Sigma^{2}$ are the eigenvalues of $AA^{T}$. | . 2.2 The Steps to compute the SVD of the Matrix A . There are four key steps to compute the SVD of the matrix which is shown below. We are going to explain each step using a random example. . Before computing the SVD of the matrix $A$, we aer going to generate a random $4 times 4$ matrix $A$. . import numpy as np import numpy.linalg as la . random_state = 42 np.random.seed(random_state) n = 4 A = np.random.randn(n, n) A . array([[ 0.49671415, -0.1382643 , 0.64768854, 1.52302986], [-0.23415337, -0.23413696, 1.57921282, 0.76743473], [-0.46947439, 0.54256004, -0.46341769, -0.46572975], [ 0.24196227, -1.91328024, -1.72491783, -0.56228753]]) . STEP 1: Find the transpose of the matrix A and compute the $A^{T}A$ . For this step, we can directly calculate the transpose of the matrix and multiply it by A using the knowledge of matrix multiplication. . A_transpose = A.T A_transpose . array([[ 0.49671415, -0.23415337, -0.46947439, 0.24196227], [-0.1382643 , -0.23413696, 0.54256004, -1.91328024], [ 0.64768854, 1.57921282, -0.46341769, -1.72491783], [ 1.52302986, 0.76743473, -0.46572975, -0.56228753]]) . ATA = A_transpose.dot(A) ATA . array([[ 0.58050469, -0.73151355, -0.24786425, 0.65940888], [-0.73151355, 4.02894983, 2.589515 , 0.43286178], [-0.24786425, 2.589515 , 6.10351105, 3.38411893], [ 0.65940888, 0.43286178, 3.38411893, 3.44164748]]) . STEP 2: Calculate the eigenpairs of $A^{T}A$ and the $V$ matrix and check its orthonormality. . For this step, we can use linear algebra package in the Python to calculate the eigenvalues and eigenvectors, . where eigenvals, eigenvecs = la.eig($A^{T}A$), eigenvecs $= left[ begin{matrix} | &amp; ldots &amp; | v_1 &amp; ldots &amp; v_n | &amp; ldots &amp; | end{matrix} right]$, and eigenvals = $ left[ begin{matrix} lambda_1 &amp; lambda_2 &amp; ldots &amp; lambda_n end{matrix} right]$ . eigenvals, eigenvecs = la.eigh(ATA) eigenvals . array([0.11672408, 0.84637788, 3.70764037, 9.48387072]) . V = eigenvecs V . array([[ 0.83813082, 0.46067627, -0.29132142, 0.02111699], [ 0.01667468, 0.47881258, 0.77530415, -0.41153852], [ 0.2888667 , -0.54399272, -0.08592964, -0.783099 ], [-0.46240103, 0.51243322, -0.55376115, -0.4657747 ]]) . Check that eigenvectors are orthonormal: . V.T @ V . array([[ 1.00000000e+00, -8.32667268e-17, 5.55111512e-17, -2.77555756e-17], [-8.32667268e-17, 1.00000000e+00, -1.24900090e-16, 1.11022302e-16], [ 5.55111512e-17, -1.24900090e-16, 1.00000000e+00, -7.63278329e-17], [-2.77555756e-17, 1.11022302e-16, -7.63278329e-17, 1.00000000e+00]]) . STEP 3: Calcualte the $ Sigma$ matrix . For this step, we can construct the diagonal matrix to get $ Sigma$. . Σ = np.diag(np.sqrt(eigenvals)) Σ . array([[0.34164905, 0. , 0. , 0. ], [0. , 0.91998798, 0. , 0. ], [0. , 0. , 1.9255234 , 0. ], [0. , 0. , 0. , 3.07958938]]) . STEP 4: Calcualte the $U$ matrix . Considering that $A = U Sigma V^{T}$, we can get $$AV = U Sigma V^{T}V$$ . Since $V^{T}V = I$ and $ Sigma^{T} Sigma = I$ . $$AV = U Sigma$$ . $$AV Sigma^{-1} = U Sigma Sigma^{-1} = U$$ . $$U = AV Sigma^{-1} $$ . U = A @ V @ la.inv(Σ) U . array([[-0.30191507, 0.64211164, -0.59773398, -0.37316755], [-0.28929081, -0.74544218, -0.3500296 , -0.48796113], [-0.8867168 , 0.06190224, 0.44410852, 0.11255683], [-0.19720909, -0.1678592 , -0.56829657, 0.78100632]]) . Check the orthogonality of U: . U @ U.T . array([[ 1.00000000e+00, -1.11022302e-16, -2.12330153e-15, -8.32667268e-16], [-1.11022302e-16, 1.00000000e+00, -2.35922393e-16, -1.66533454e-16], [-2.12330153e-15, -2.35922393e-16, 1.00000000e+00, -7.49400542e-16], [-8.32667268e-16, -1.66533454e-16, -7.49400542e-16, 1.00000000e+00]]) . STEP 5: Compare with the Python SVD. . U1, Σ1, V1t = la.svd(A) . U1, U . (array([[-0.37316755, -0.59773398, 0.64211164, -0.30191507], [-0.48796113, -0.3500296 , -0.74544218, -0.28929081], [ 0.11255683, 0.44410852, 0.06190224, -0.8867168 ], [ 0.78100632, -0.56829657, -0.1678592 , -0.19720909]]), array([[-0.30191507, 0.64211164, -0.59773398, -0.37316755], [-0.28929081, -0.74544218, -0.3500296 , -0.48796113], [-0.8867168 , 0.06190224, 0.44410852, 0.11255683], [-0.19720909, -0.1678592 , -0.56829657, 0.78100632]])) . Σ1, Σ . (array([3.07958938, 1.9255234 , 0.91998798, 0.34164905]), array([[0.34164905, 0. , 0. , 0. ], [0. , 0.91998798, 0. , 0. ], [0. , 0. , 1.9255234 , 0. ], [0. , 0. , 0. , 3.07958938]])) . V1t.T, V . (array([[ 0.02111699, -0.29132142, 0.46067627, 0.83813082], [-0.41153852, 0.77530415, 0.47881258, 0.01667468], [-0.783099 , -0.08592964, -0.54399272, 0.2888667 ], [-0.4657747 , -0.55376115, 0.51243322, -0.46240103]]), array([[ 0.83813082, 0.46067627, -0.29132142, 0.02111699], [ 0.01667468, 0.47881258, 0.77530415, -0.41153852], [ 0.2888667 , -0.54399272, -0.08592964, -0.783099 ], [-0.46240103, 0.51243322, -0.55376115, -0.4657747 ]])) . As we can see above we can get the same result using two different methods. In addition, the matrices $U$ and $V$ are not singular due to orthogonality. . 2.3 Second, Assume that A is not a Square Matrix (m &gt; n or m &lt; n) . Reduced SVD . First Case(m &gt; n): . In this case, we can see that there are n singular values. The $ Sigma$ is a $n times n$ diagonal matrix with positive real entries, $U$ is a $m times n$ matrix with orthonormal columns, and $V$ is a $n times n$ matrix with orthonormal columns. . Second Case(m &lt; n): . In this case, we can see that there are m singular values. The $ Sigma$ is a $m times m$ diagonal matrix with positive real entries, $U$ is a $m times m$ matrix with orthonormal columns, and $V$ is a $m times n$ matrix with orthonormal columns. . The singular value decomposition of the $m times n$ matirx $A$ is caleed as Reduced Singular Value Decomposition: . $$A = U Sigma V^{T}$$ . where $A$ is a $m times n$ matrix, $U$ is a $m times k$ matrix, $ Sigma$ is a $k times k$ matrix, $V^{T}$ ia a $k times n$ matrix, and $k$ = min($m$,$n$). . Therefore, we can conclude that: . Full Rank:$A = underset{m times m}U$ $ underset{m times n} Sigma$ $ underset{n times n} V^{T}$ . Reduced Rank: $A = underset{m times k}U$ $ underset{k times k} Sigma$ $ underset{k times n} V^{T}$ where $k$ = min($m$, $n$) . 3. The Computation Cost of SVD . The Computational cost of SVD is calculated by a two-step process. In the first step, assume that $m geq n$, the matrix is reduced to a bidiagonal matrix using QR decomposition and Householder reflections which in overall take $2mn^{2} + 2n^{3}$. In the second step, the iterative method is applied to compute the SVD of the bidiagonal matrix, where the cost is O(n) which is less expensive than the first step. . As shown above, the cost of the SVD is proportional to $ beta(mn^{2}+n^{3})$ and the $ beta$ is ranging from 4 to 10 or more with respect to different algorithm. . In conclusion, . Full Rank$m = n$: cost $ rightarrow$ $n cdot n^{2} + n^{3} = 2n^{3}$ . Reduced Rank: $m &gt; n$: cost $ geq$ $2n^{3}$ . 4. SVD APPLICATIONS . 4.1 Pseudo-inverse and SVD . In linear algebra, for any $m times n$ matrix $A$, there exists a unique $n times m$ matrix $A^{ dagger}$ which can be computed using Singular Value Decomposition . Consider A be any an $m times n$ of rank r with a singular value decomposition $A = U Sigma V^{T}$ with nonzero singular value $ sigma_1 geq sigma_2 geq ldots geq sigma_r$. Then we can get the pseudoinverse of A: . $$A^{ dagger} = V Sigma U^{ star}$$ . Where the $ Sigma^{ dagger}$ is a $n times n$ matrix which can be computed by taking the reciprocal of all non-zero elements and then leaving all zero value alone. . Example 1: Suppose $A$ = $ left[ begin{matrix} 1 &amp; 1 1 &amp;1 -1&amp;-1 end{matrix} right]$, we can use Python to calculate SVD which is equal to . $$ left[ begin{matrix} -0.577350269 &amp; 0.816496581 &amp; 0 -0.577350269 &amp; -0.408248290 &amp; -0.707106781 -0.577350269 &amp; 0.408248290 &amp; -0.707106781 end{matrix} right] left[ begin{matrix} sqrt{6} &amp; 0 0 &amp; 0 0 &amp; 0 end{matrix} right] left[ begin{matrix} -0.70710678 &amp; 0.70710678 -0.70710678 &amp; -0.70710678 end{matrix} right]^{T}$$ . Then we can calculate the $A^{ dagger}$ by using the equation above: . $$A^{ dagger} = left[ begin{matrix} -0.70710678 &amp; 0.70710678 -0.70710678 &amp; -0.70710678 end{matrix} right] left[ begin{matrix} frac{1}{ sqrt{6}} &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 end{matrix} right] left[ begin{matrix} -0.577350269 &amp; 0.816496581 &amp; 0 -0.577350269 &amp; -0.408248290 &amp; -0.707106781 -0.577350269 &amp; 0.408248290 &amp; -0.707106781 end{matrix} right]^{T}$$ $$A^{ dagger} = left[ begin{matrix} frac{1}{6} &amp; frac{1}{6} &amp; - frac{1}{6} frac{1}{6} &amp; frac{1}{6} &amp; - frac{1}{6} end{matrix} right]$$ . which is the same as below: . A = np.array([[1, 1], [1, 1], [-1, -1]]) la.pinv(A) . array([[ 0.16666667, 0.16666667, -0.16666667], [ 0.16666667, 0.16666667, -0.16666667]]) . 4.2 Systems of Linear Equations and SVD . Consider a system of linear equations $Ax = b$ where A is an $m times n$ matrix and $b in F^{m}$，there are three possibilities for this system: . it has no solutions | it has a unique solutions | it has infinitely many solutions | . By Theorem, . If $Ax = b$ is consistent and $A$ is invertible, then $A^{-1} = A^{ dagger}$. The solution can be written as $x = A^{ dagger}b$, where $x$ is the unique solution to the system having minimum norm. . | f $Ax = b$ is inconsistent or $A$ is not invertible, then $A^{ dagger}b$ still exists. The solution can be written as $x = A^{ dagger}b$, where $x$ is the unique best approximation to the system having minimum norm. . | . Example 2: Consider two linear systems . $ left { begin{array}{rcl} x_1 + x_2 = 2 x_1 + x_2 = 2 -x_1 - x_2 = -2 end{array} right.$ $ and $ $ left { begin{array}{rcl} x_1 + x_2 = 1 x_1 + x_2 = 2 -x_1 - x_2 = 2 end{array} right.$ . Obviously, the first system has infinitely many solutions. Let $A = left[ begin{matrix} 1 &amp; 1 1 &amp;1 -1&amp;-1 end{matrix} right]$ which is the coefficient matrix of the system and $b = left[ begin{matrix} 2 2 2 end{matrix} right].$ Using the example 1 above, $A^{ dagger} = left[ begin{matrix} frac{1}{6} &amp; frac{1}{6} &amp; - frac{1}{6} frac{1}{6} &amp; frac{1}{6} &amp; - frac{1}{6} end{matrix} right]$. Thus, $x = A^{ dagger}b = left[ begin{matrix} frac{1}{6} &amp; frac{1}{6} &amp; - frac{1}{6} frac{1}{6} &amp; frac{1}{6} &amp; - frac{1}{6} end{matrix} right] left[ begin{matrix} 2 2 2 end{matrix} right] = left[ begin{matrix} frac{1}{3} frac{1}{3} end{matrix} right] $, which is the solution of minimal norm. . On the other hand, the second system which is inconsistent has no solution. However, $x = A^{ dagger}b = left[ begin{matrix} frac{1}{6} &amp; frac{1}{6} &amp; - frac{1}{6} frac{1}{6} &amp; frac{1}{6} &amp; - frac{1}{6} end{matrix} right] left[ begin{matrix} 1 2 2 end{matrix} right] = left[ begin{matrix} frac{1}{6} frac{1}{6} end{matrix} right] $ is the &quot;best approximation&quot; to this solution with minimum norm. . 4.3 Image Compression and SVD . 4.3.1 Low-Rank Approximation from the SVD . Assume that $m &gt; n$, for any $m times n$ matrix, there exists another way to represent the SVD: . $$A = left[ begin{matrix} vdots &amp; ldots &amp; vdots u_1 &amp; ldots &amp; u_m vdots &amp; ldots &amp; vdots end{matrix} right] left[ begin{matrix} sigma_1 &amp; &amp; &amp; ddots &amp; &amp; &amp; sigma_n &amp;&amp;0 &amp;&amp; vdots &amp;&amp;0 end{matrix} right] left[ begin{matrix} ldots &amp; v_1^{T} &amp; ldots vdots &amp; vdots &amp; vdots ldots &amp; v_n^{T} &amp; ldots end{matrix} right]$$ . $$A = left[ begin{matrix} vdots &amp; ldots &amp; vdots u_1 &amp; ldots &amp; u_m vdots &amp; ldots &amp; vdots end{matrix} right] left[ begin{matrix} ldots &amp; sigma_1 v_1^{T} &amp; ldots vdots &amp; vdots &amp; vdots ldots &amp; sigma_n v_n^{T} &amp; ldots end{matrix} right] = sigma_1 u_1 v_1^{T} + sigma_2 u_2 v_2^{T} + ldots + sigma_n u_n v_n^{T} $$ . where $ sigma_1 geq sigma_2 geq sigma_3 geq ldots geq 0$. . Therefore, in order to make the best rank-k approximation of $A$, where $k leq$ min($m$,$n$), we have to minimize $ underset{A_k} {min} | A - A_k |$ s.t rank($A_k$) $ leq k$. Then we keep the top k right singular vectors corresponding to the first k rows of $V^{T}$ which is denoted as $V_k^{T}$. Similarly, we also keep the top k left singular vectors corresponding to the first k columns of $U$ which is denoted as $U_k$. Last, we only keep the top k singular values and then we can get: . $$A_k = sigma_1 u_1 v_1^{T} + sigma_2 u_2 v_2^{T} + ldots + sigma_k u_k v_k^{T}$$ . where $ sigma_1 geq sigma_2 geq sigma_3 geq ldots geq 0$ and the norm of the difference between $A$ and $A_k$ is equal to $ | A - A_k |_2 = | sum_{i=k+1}^{n} sigma_i u_i v_i^{T} | = sigma_{k+1}.$ . 4.3.2 Image Compression . Images from the real world have full rank, but there are only a small number of the singular values in the SVD of images are significantly large. In image compression, a low-rank approximation of the matrix is applied to approximate images. Suppose the $m times n$ matrix $A$ is a 2-d image, instead of storing all m rows and n columns of the original matrix, the image is compressed by selecting r singular vectors and values. Then the compressed image $A$ is approximated as the product of a handful of r columns and r rows which is known as the low-rank approximation. . In the image processing, insteads of using three channel of color(Red, Green, Blue), the grayscale image is preferred in which the only colors are the shades of gray. The reason is that the gray is the color in which all RGB have equal intensity and therefore rather than using three intensities to speicify each pixel it&#39;s only necessary to include a single intensity value. In addition, in the real world, grayscale intensity is stores as 8-bit int which could provide 256 different shades of gray. Therefore, given that the image capture hardwares are only able to support 8-bits images, grayscale images are very popular and sufficient for many image processings. . Example 3: There is an example to illustarte how SVD applies to the image compression. . First Step: Import required packages and images and convert the image to grayscale. . import matplotlib.pyplot as plt import numpy as np %matplotlib inline from PIL import Image . with Image.open(&quot;images Kitten.jpg&quot;) as img: rgb_img = np.array(img) print(rgb_img.shape) plt.figure(figsize = (30, 20)) plt.title(&quot;Original Image&quot;, size = 35) plt.imshow(rgb_img) . (3024, 4032, 3) . &lt;matplotlib.image.AxesImage at 0x27fcc2b5b38&gt; . A = np.sum(rgb_img, axis=-1) print(A.shape) plt.figure(figsize = (30, 20)) plt.title(&quot;Before Compression&quot;, size = 35) plt.imshow(A, cmap = &#39;gray&#39;) . (3024, 4032) . &lt;matplotlib.image.AxesImage at 0x27fce86a7f0&gt; . Second Step: Perform SVD to the grayscale image. . U, Σ, Vt = la.svd(A) print(&quot;U: &quot;, U.shape) print(&quot;Σ: &quot;, Σ.shape) print(&quot;Vt: &quot;, Vt.shape) . U: (3024, 3024) Σ: (3024,) Vt: (4032, 4032) . Third Step: Analyze the relationship between singular values and its indices. . Analyze how the the singular values change with respect to the singular value index. . plt.figure(figsize = (10, 6)) plt.loglog(Σ, lw=3, color = &#39;red&#39;) plt.xlabel(&#39;Singular Value Index&#39;, size = 16) plt.ylabel(&#39;Singular Values&#39;, size = 16) plt.title(&quot;Singular Values vs Singular Value Index&quot;, size = 20) . Text(0.5, 1.0, &#39;Singular Values vs Singular Value Index&#39;) . Forth Step: Select k significant singular values and vectors. . index = 1 fig = plt.figure(figsize = (15,10)) for i in range(20, 1500, 400): compressed_img = np.matrix(U[:, :i]) * np.diag(Σ[:i]) * np.matrix(Vt[:i,:]) plt.subplot(2, 2, index) plt.imshow(compressed_img, cmap = &#39;gray&#39;) title = &quot;Compressed Image: k = %s&quot; %i plt.title(title) print(&quot;k = &quot;, i) print(&quot;original size: %d&quot; % rgb_img.size) compressed_size = U[:,:i].size + Σ[:i].size + Vt[:i,:].size print(&quot;compressed size: %d&quot; % compressed_size) print(&quot;ratio: %f&quot; % (compressed_size / rgb_img.size)) index += 1 plt.show() . k = 20 original size: 36578304 compressed size: 141140 ratio: 0.003859 k = 420 original size: 36578304 compressed size: 2963940 ratio: 0.081030 k = 820 original size: 36578304 compressed size: 5786740 ratio: 0.158201 k = 1220 original size: 36578304 compressed size: 8609540 ratio: 0.235373 . 4.4 PCA and SVD . Principal component analysis (PCA) and SVD are both widely used in the machine learning and analysis of multivariate data in order to reduce dimensionality. For PCA, it&#39;s always used to reduce the feature space but keeps the most relevant and important information about features. It calculates the directions of maximum variance in high-dimensional data and projects it into a smaller dimensional subspace. PCA and SVD has a close relationship: we can use the SVD to conduct principal comonent analysis and vice versa. . Example 4: There is an example to illustarte how SVD and PCA are combined to reduce the dimension of a dataset while keeping important information about features. . First Step: Import required packages and datasets and perform EDA. . import seaborn as sns import pandas as pd df = sns.load_dataset(&quot;iris&quot;) print(df.shape) df.head() . (150, 5) . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . idx = df.columns d = {&#39;mean&#39;:df.mean(), &#39;Standard Deviation&#39;: df.std()} df1 = pd.DataFrame(data = d, index = idx) df1 . mean Standard Deviation . sepal_length 5.843333 | 0.828066 | . sepal_width 3.057333 | 0.435866 | . petal_length 3.758000 | 1.765298 | . petal_width 1.199333 | 0.762238 | . species NaN | NaN | . Second Step: Standardize the dataset to zero mean and compute the svd of this dataset. . df_shifted = df.iloc[:,:4] - df.iloc[:,:4].mean() df_standardized = df_shifted / df.iloc[:,:4].std() #compute the SVD U, Σ, Vt = la.svd(df_standardized, full_matrices = False) print(&quot;U: &quot;, U.shape) print(&quot;Σ: &quot;, Σ.shape, Σ) print(&quot;Vt: &quot;, Vt.shape) print(&quot;Variances: &quot;, Σ**2) . U: (150, 4) Σ: (4,) [20.85320538 11.67007028 4.6761923 1.75684679] Vt: (4, 4) Variances: [434.85617466 136.19054025 21.86677446 3.08651063] . Third Step: Analyze the relationship between principal component and vairances about each feature. . tot = sum(Σ**2) variance_ratio = [(i / tot)*100 for i in Σ**2] variance_cumulative= np.cumsum(variance_ratio) print(variance_ratio) print(variance_cumulative) fig = plt.figure(figsize = (10,6)) plt.bar(range(4), variance_ratio, alpha=0.5, align=&#39;center&#39;, label=&#39;Individual Explained Variance&#39;, color = &#39;darkblue&#39;) plt.step(range(4), variance_cumulative, where=&#39;mid&#39;, label=&#39;Cumulative Explained Variance&#39;, color = &#39;darkred&#39;) plt.ylabel(&#39;Explained Variance Ratio&#39;) plt.xlabel(&#39;Principal Components&#39;) plt.xticks(np.arange(0, 4, step=1)) plt.yticks(np.arange(0, 101, step=10)) plt.legend() plt.tight_layout() . [72.9624454132999, 22.85076178670173, 3.668921889282877, 0.5178709107154799] [ 72.96244541 95.8132072 99.48212909 100. ] . This plot shows that around 73% of the variance can be explained by the first principal component. Around 95.8% of the variance can be explained by the first two principal components. Therefore, the remaining principal components contains few information about features which could be ignored. . Forth Step: Perform PCA. . V = Vt.T Zstar = df_standardized@V[:,:2] fig = plt.figure(figsize = (10,6)) df[&#39;p0&#39;] = Zstar.iloc[:,0] df[&#39;p1&#39;] = Zstar.iloc[:,1] fig = plt.figure(figsize = (10,6)) g1 = sns.lmplot(&#39;p0&#39;, &#39;p1&#39;, df, hue=&#39;species&#39;, fit_reg=False, size=10, scatter_kws={&#39;alpha&#39;:0.7,&#39;s&#39;:60}) ax = g1.axes[0,0] ax.axhline(y=0, color=&#39;k&#39;) ax.axvline(x=0, color=&#39;k&#39;) . D: Anaconda lib site-packages seaborn regression.py:546: UserWarning: The `size` paramter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . &lt;matplotlib.lines.Line2D at 0x1d5314bc908&gt; . &lt;Figure size 720x432 with 0 Axes&gt; . &lt;Figure size 720x432 with 0 Axes&gt; . plt.figure(figsize = (10,6)) plt.subplot(1, 2, 1) plt.bar(df.columns[:4],V[:,0]) plt.xticks(rotation=90) plt.title(&#39;influence of original features in p0&#39;) plt.subplot(1, 2, 2) plt.bar(df.columns[:4],V[:,1]) plt.xticks(rotation=90) plt.title(&#39;influence of original features in p1&#39;) . Text(0.5, 1.0, &#39;influence of original features in p1&#39;) . REFERENCE . CS357: Spring 2019. (n.d.). Retrieved November 21, 2020, from https://relate.cs.illinois.edu/course/cs357-s19/page/demos/ | . Singular value decomposition. (2020, November 09). Retrieved November 21, 2020, from https://en.wikipedia.org/wiki/Singular_value_decomposition | . John. (2018, May 05). Computing SVD and pseudoinverse. Retrieved November 21, 2020, from https://www.johndcook.com/blog/2018/05/05/svd/ | . Grayscale Images. (n.d.). Retrieved November 21, 2020, from https://homepages.inf.ed.ac.uk/rbf/HIPR2/gryimage.htm | . Putalapattu, R. (2017, August 20). Jupyter, python, Image compression and svd - An interactive exploration. Retrieved November 21, 2020, from https://medium.com/@rameshputalapattu/jupyter-python-image-compression-and-svd-an-interactive-exploration-703c953e44f6 | . Wang, Z. (2019, September 05). PCA and SVD explained with numpy. Retrieved November 21, 2020, from https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8 | .",
            "url": "https://yunxuanzeng.github.io/data1010_Project/2020/11/21/Applications-of-SVD.html",
            "relUrl": "/2020/11/21/Applications-of-SVD.html",
            "date": " • Nov 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://yunxuanzeng.github.io/data1010_Project/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://yunxuanzeng.github.io/data1010_Project/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://yunxuanzeng.github.io/data1010_Project/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yunxuanzeng.github.io/data1010_Project/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}